import{_ as a}from"./_plugin-vue_export-helper.cdc0426e.js";import{o as n,c as s,a as e,b as t,d,w as i,e as r,r as o}from"./app.7342ef47.js";const f={},y=e("h1",{id:"streamingfile-流式hdfs-连接器",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#streamingfile-流式hdfs-连接器","aria-hidden":"true"},"#"),t(" StreamingFile(流式HDFS)连接器")],-1),g=r('<p><strong>StreamingFile</strong> 连接器主要使用于流式场景中，提供以 <code>Exactly-Once</code> 语义的写入 HDFS、Hive 的能力，为实时数仓提供可靠的保障。</p><h2 id="主要功能" tabindex="-1"><a class="header-anchor" href="#主要功能" aria-hidden="true">#</a> 主要功能</h2><ul><li>写入支持<code>exactly once</code>。</li><li>提供多种提交策略，能够兼容数据完整性优先和数据时效性优先。</li><li>数据触发，有效解决延迟数据带来的数据漂移问题。</li><li>Hive表结构自动发现，解决表结构变更，任务未及时重启的导致的数据不一致；</li></ul><h2 id="支持的数据类型" tabindex="-1"><a class="header-anchor" href="#支持的数据类型" aria-hidden="true">#</a> 支持的数据类型</h2><ul><li>HDFS <ul><li>无需关心数据数据结构；直接写入读取到的字节数组。</li></ul></li><li>HIVE <ul><li>基础类型 <ul><li>TINYINT</li><li>SMALLINT</li><li>INT</li><li>BIGINT</li><li>BOOLEAN</li><li>FLOAT</li><li>DOUBLE</li><li>STRING</li><li>BINARY</li><li>TIMESTAMP</li><li>DECIMAL</li><li>CHAR</li><li>VARCHAR</li><li>DATE</li></ul></li><li>复杂类型 <ul><li>Array</li><li>Map</li></ul></li></ul></li></ul><h2 id="主要参数" tabindex="-1"><a class="header-anchor" href="#主要参数" aria-hidden="true">#</a> 主要参数</h2><h3 id="通用参数" tabindex="-1"><a class="header-anchor" href="#通用参数" aria-hidden="true">#</a> 通用参数</h3><table><thead><tr><th style="text-align:left;">参数名称</th><th style="text-align:left;">参数是否必须</th><th style="text-align:left;">参数默认值</th><th style="text-align:left;">参数枚举值</th><th style="text-align:left;">参数含义</th></tr></thead><tbody><tr><td style="text-align:left;">class</td><td style="text-align:left;">是</td><td style="text-align:left;">-</td><td style="text-align:left;"></td><td style="text-align:left;">com.bytedance.bitsail.connector.legacy.streamingfile.sink.FileSystemSinkFunctionDAGBuilder</td></tr><tr><td style="text-align:left;">dump.format.type</td><td style="text-align:left;">是</td><td style="text-align:left;">-</td><td style="text-align:left;">hdfs<br>hive<br></td><td style="text-align:left;">写入哪种存储类型: HDFS 或者 Hive</td></tr></tbody></table><h3 id="通用优化参数" tabindex="-1"><a class="header-anchor" href="#通用优化参数" aria-hidden="true">#</a> 通用优化参数</h3><table><thead><tr><th style="text-align:left;">参数名称</th><th style="text-align:left;">参数是否必须</th><th style="text-align:left;">参数默认值</th><th style="text-align:left;">参数枚举值</th><th style="text-align:left;">参数含义</th></tr></thead><tbody><tr><td style="text-align:left;">enable_event_time</td><td style="text-align:left;">否</td><td style="text-align:left;">False</td><td style="text-align:left;"></td><td style="text-align:left;">是否开启归档</td></tr><tr><td style="text-align:left;">event_time_fields</td><td style="text-align:left;">否</td><td style="text-align:left;">-</td><td style="text-align:left;"></td><td style="text-align:left;">如果开启，指明归档字段的名称，这个名称是指字段在原始结构中的名称。</td></tr><tr><td style="text-align:left;">event_time_pattern</td><td style="text-align:left;">否</td><td style="text-align:left;">-</td><td style="text-align:left;"></td><td style="text-align:left;">如果该字段为空，则按照unix时间戳进行解析；如果该字段不为空，则按照该字段指定的格式进行解析，例如&quot;yyyy-MM-dd HH:mm:ss&quot;</td></tr><tr><td style="text-align:left;">event_time.tag_duration</td><td style="text-align:left;">否</td><td style="text-align:left;">900000</td><td style="text-align:left;"></td><td style="text-align:left;">单位:milliseconds，用于描述归档最大等待时间，计算公式为:当前event_time - 归档标签的时间 &gt; event_time.tag_duration 就会生成这个小时的标签。例如，业务时间为:9:45，tag_duration=40min, 待生成的小时标签为8:00 9:45 - (8:00 + 60min) = 45min &gt; 40min，则可以生成8点标签60min为默认需要等待一个小时才能打标签40min为需要额外等待的时间</td></tr><tr><td style="text-align:left;">dump.directory_frequency</td><td style="text-align:left;">否</td><td style="text-align:left;">dump.directory_frequency.day</td><td style="text-align:left;">dump.directory_frequency.day<br>dump.directory_frequency.hour</td><td style="text-align:left;">输出格式为hdfs时，指定目录切分方式，dump.directory_frequency.day：按照日期进行目录划分dump.directory_frequency.hour：按照小时进行目录切分</td></tr><tr><td style="text-align:left;">rolling.inactivity_interval</td><td style="text-align:left;">否</td><td style="text-align:left;">-</td><td style="text-align:left;"></td><td style="text-align:left;">单文件距离上次写入间隔指定切分文件</td></tr><tr><td style="text-align:left;">rolling.max_part_size</td><td style="text-align:left;">否</td><td style="text-align:left;">-</td><td style="text-align:left;"></td><td style="text-align:left;">单文件达到指定写入大小切分文件</td></tr><tr><td style="text-align:left;">partition_strategy</td><td style="text-align:left;">否</td><td style="text-align:left;">partition_last</td><td style="text-align:left;">partition_first,partition_last</td><td style="text-align:left;">Hive 添加分区策略，支持 partition_last 和 partition_first 两种策略partition_last： 等分区所有数据都写入到hive后才添加hive 分区，添加分区延迟为天级任务 1 天，小时级任务 1 小时。partition_first：先加分区，适用于准实时场景，添加分区延迟为 1 个 Checkpoint 的间隔。</td></tr></tbody></table><h3 id="hdfs-参数" tabindex="-1"><a class="header-anchor" href="#hdfs-参数" aria-hidden="true">#</a> HDFS 参数</h3><table><thead><tr><th style="text-align:left;">参数名称</th><th style="text-align:left;">参数是否必须</th><th style="text-align:left;">参数默认值</th><th style="text-align:left;">参数枚举值</th><th style="text-align:left;">参数含义</th></tr></thead><tbody><tr><td style="text-align:left;">dump.output_dir</td><td style="text-align:left;">是</td><td style="text-align:left;">-</td><td style="text-align:left;"></td><td style="text-align:left;">输出格式为hdfs时，指定hdfs的输出路径。</td></tr><tr><td style="text-align:left;">hdfs.dump_type</td><td style="text-align:left;">是</td><td style="text-align:left;">-</td><td style="text-align:left;">hdfs.dump_type.text:文本格式<br>hdfs.dump_type.json: json格式<br>hdfs.dump_type.msgpack: msgpack格式<br>hdfs.dump_type.binary: 通过protobuf进行解析,需要配合proto.descriptor和proto.class_name参数。</td><td style="text-align:left;">解析的数据格式，按照需求进行填写</td></tr><tr><td style="text-align:left;">partition_infos</td><td style="text-align:left;">是</td><td style="text-align:left;">-</td><td style="text-align:left;"></td><td style="text-align:left;">写入hdfs的分区结构信息，对于hdfs来说，只可以是[{&quot;name&quot;:&quot;date&quot;,&quot;value&quot;:&quot;yyyyMMdd&quot;,&quot;type&quot;:&quot;TIME&quot;},{&quot;name&quot;:&quot;hour&quot;,&quot;value&quot;:&quot;HH&quot;,&quot;type&quot;:&quot;TIME&quot;}]</td></tr><tr><td style="text-align:left;">hdfs.replication</td><td style="text-align:left;">否</td><td style="text-align:left;">3</td><td style="text-align:left;"></td><td style="text-align:left;">hdfs输出副本数量</td></tr><tr><td style="text-align:left;">hdfs.compression_codec</td><td style="text-align:left;">否</td><td style="text-align:left;">None</td><td style="text-align:left;"></td><td style="text-align:left;">hdfs压缩格式，请先确定当前使用的hadoop中支持哪些压缩方式后进行设置。None表示不压缩。</td></tr><tr><td style="text-align:left;">hdfs.overwrite</td><td style="text-align:left;">否</td><td style="text-align:left;">False</td><td style="text-align:left;"></td><td style="text-align:left;">是否覆盖目标路径下原有文件。</td></tr></tbody></table><h3 id="hive参数" tabindex="-1"><a class="header-anchor" href="#hive参数" aria-hidden="true">#</a> HIVE参数</h3><table><thead><tr><th style="text-align:left;">参数名称</th><th style="text-align:left;">参数是否必须</th><th style="text-align:left;">参数默认值</th><th style="text-align:left;">参数枚举值</th><th style="text-align:left;">参数含义</th></tr></thead><tbody><tr><td style="text-align:left;">db_name</td><td style="text-align:left;">是</td><td style="text-align:left;">-</td><td style="text-align:left;"></td><td style="text-align:left;">写入Hive库名</td></tr><tr><td style="text-align:left;">table_name</td><td style="text-align:left;">是</td><td style="text-align:left;">-</td><td style="text-align:left;"></td><td style="text-align:left;">写入Hive表名</td></tr><tr><td style="text-align:left;">metastore_properties</td><td style="text-align:left;">是</td><td style="text-align:left;">-</td><td style="text-align:left;"></td><td style="text-align:left;">Hive metastore的配置，包括url连接，以及一些其它可选配置。</td></tr><tr><td style="text-align:left;">source_schema</td><td style="text-align:left;">是</td><td style="text-align:left;">-</td><td style="text-align:left;"></td><td style="text-align:left;">原始Schema信息，为string类型；例如[{&quot;name&quot;:&quot;id&quot;,&quot;type&quot;:&quot;bigint&quot;},{&quot;name&quot;:&quot;user_name&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;create_time&quot;,&quot;type&quot;:&quot;bigint&quot;}]</td></tr><tr><td style="text-align:left;">sink_schema</td><td style="text-align:left;">是</td><td style="text-align:left;">-</td><td style="text-align:left;"></td><td style="text-align:left;">目标Schema信息，为string类型；例如[{&quot;name&quot;:&quot;id&quot;,&quot;type&quot;:&quot;bigint&quot;},{&quot;name&quot;:&quot;user_name&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;create_time&quot;,&quot;type&quot;:&quot;bigint&quot;}]</td></tr><tr><td style="text-align:left;">partition_infos</td><td style="text-align:left;">是</td><td style="text-align:left;">-</td><td style="text-align:left;"></td><td style="text-align:left;">写入hive的分区结构信息，对于hive来说，可以按照分区来进行填写，如果是存在小时级别分区，示例为[{&quot;name&quot;:&quot;date&quot;,&quot;type&quot;:&quot;TIME&quot;},{&quot;name&quot;:&quot;hour&quot;,&quot;type&quot;:&quot;TIME&quot;}]</td></tr><tr><td style="text-align:left;">hdfs.dump_type</td><td style="text-align:left;">是</td><td style="text-align:left;">-</td><td style="text-align:left;">hdfs.dump_type.text:文本格式<br>hdfs.dump_type.json: json格式<br>hdfs.dump_type.msgpack: msgpack格式<br>hdfs.dump_type.binary: 通过protobuf进行解析,需要配合proto.descriptor和proto.class_name参数。</td><td style="text-align:left;">解析的数据格式，按照需求进行填写</td></tr></tbody></table><h2 id="相关文档" tabindex="-1"><a class="header-anchor" href="#相关文档" aria-hidden="true">#</a> 相关文档</h2>',15);function u(x,h){const l=o("RouterLink");return n(),s("div",null,[y,e("p",null,[t("上级文档："),d(l,{to:"/zh/documents/connectors/"},{default:i(()=>[t("连接器")]),_:1})]),g,e("p",null,[t("配置示例文档："),d(l,{to:"/zh/documents/connectors/streamingfile/streamingfile_example.html"},{default:i(()=>[t("StreamingFile(流式HDFS)连接器")]),_:1})])])}const m=a(f,[["render",u],["__file","streamingfile.html.vue"]]);export{m as default};
